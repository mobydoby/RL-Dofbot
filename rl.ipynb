{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib created a temporary config/cache directory at /tmp/matplotlib-yunr5gyn because the default path (/home/jetson/.config/matplotlib) is not a writable directory; it is highly recommended to set the MPLCONFIGDIR environment variable to a writable directory, in particular to speed up the import of Matplotlib and to better support multiprocessing.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Arm_Lib import Arm_Device\n",
    "real_arm = Arm_Device()  # Get DOFBOT object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up matplotlib\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defines Arm related functions for moving the arm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_angles(arm, angles, sec_per_angle):\n",
    "    \"\"\"Sets all the angles of arm\n",
    "    Args:\n",
    "        arm - the real robot\n",
    "        angles - the angles to set\n",
    "        t - the amount of time to move each angle\n",
    "    \"\"\"\n",
    "    angles = (angles * 180/np.pi).astype(int)\n",
    "    print(\"here\", angles)\n",
    "    t = angles*sec_per_angle\n",
    "    for joint, (angle, movetime) in enumerate(zip(angles, t)):\n",
    "        time.sleep(1)\n",
    "        print(f\"Command: Joint {joint+1} -> {angle} deg in {movetime} ms\")\n",
    "        arm.Arm_serial_servo_write(joint+1, angle, movetime)\n",
    "        time.sleep(1)\n",
    "        \n",
    "def read_all_joints_in_radians(arm):\n",
    "    q = np.array([arm.Arm_serial_servo_read(id) for id in range(1,6)])*np.pi/180\n",
    "    return q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_arm.Arm_serial_servo_write(1, 170, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda (device)\n"
     ]
    }
   ],
   "source": [
    "# search for gpu\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'Using {device} (device)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the architechure for the Actor and Critic Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Dofbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.     0.     0.1045] [0. 0. 0.] [0.08285 0.      0.     ] [-0.      -0.      -0.08285] [0. 0. 0.] [-0.12842 -0.      -0.     ]\n",
      "limits: \n",
      "[[  0 180]\n",
      " [  0 180]\n",
      " [  0 180]\n",
      " [  0 180]\n",
      " [  0 270]\n",
      " [  0 180]]\n"
     ]
    }
   ],
   "source": [
    "ex = np.array([1,0,0])\n",
    "ey = np.array([0,1,0])\n",
    "ez = np.array([0,0,1])\n",
    "l0 = 0.061 # base to servo 1\n",
    "l1 = 0.0435 # servo 1 to servo 2\n",
    "l2 = 0.08285 # servo 2 to servo 3\n",
    "l3 = 0.08285 # servo 3 to servo 4\n",
    "l4 = 0.07385 # servo 4 to servo 5\n",
    "l5 = 0.05457 # servo 5 to gripper\n",
    "P01 = ( l0 + l1 ) * ez \n",
    "P12 = np.zeros (3) # translation between 1 and 2 frame in 1 frame\n",
    "P23 = l2 * ex # translation between 2 and 3 frame in 2 frame\n",
    "P34 = - l3 * ez # translation between 3 and 4 frame in 3 frame\n",
    "P45 = np.zeros (3) # translation between 4 and 5 frame in 4 frame\n",
    "P5T = -( l4 + l5 ) * ex \n",
    "print(P01,P12,P23,P34,P45,P5T)\n",
    "\n",
    "P = np.array([P01, P12, P23, P34, P45, P5T]).T\n",
    "H = np.array([ez, -ey, -ey, -ey, -ex]).T\n",
    "limits = np.array([0,180] * 6).reshape(6, 2)\n",
    "limits[4, :] = [0, 270]\n",
    "print(f\"limits: \\n{limits}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Robot import UR5Arm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During the Training Cycle, the following steps are performed for each epoch:\n",
    "1. Initialize the Robot to a random state.\n",
    "2. select a random end effector position and orientation that\n",
    "   converges to valid joint angles using inverse kinematics\n",
    "3. Run the following in each training epoch that lasts 10 seconds per iteration\n",
    "4. The robot can takes 1 action in a discrete action space for each joint:\n",
    "   - the DQN network outputs a value between -10 and 10 and the arm is commanded to\n",
    "     move each servo that angle amount with velocity V\n",
    "5. Repeat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to establish the bounds for this robot. \n",
    "We move the robot to the home position and measure the maximal length the end effector can reach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_P_from_limits(limits, y_bound = 0.05):\n",
    "    \"\"\"Tests the target EE configuration process\n",
    "    Generates a random end effector point based on Position vector limits\n",
    "    Args:\n",
    "        limits is a tuple of the smallest and largest norms of P_ees generated\n",
    "    \"\"\"\n",
    "    match = False\n",
    "    tries = 0\n",
    "    while not match and tries<100:\n",
    "        target_norm = np.random.uniform(limits[0], limits[1])\n",
    "        print(f\"Generated Target Norm: {target_norm:.4f}\")\n",
    "        random = np.random.rand(3)\n",
    "        norm_vec = np.linalg.norm(random)\n",
    "        out = random * target_norm/norm_vec\n",
    "        print(out)\n",
    "        if out[1] > y_bound:\n",
    "            print(f\"P generated: {out}\")\n",
    "            return out \n",
    "        print(\"Retrying: y value not within bounds\")\n",
    "        tries += 1\n",
    "    print(\"No match was found, maybe the y_bound was too strict?\")\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_Q_from_limits(dofbot: UR5Arm, limits, y_bound = 0.05):\n",
    "    # generate random 5 joint angles\n",
    "    # test whether the end effector position is in the bound\n",
    "    tries = 100\n",
    "    curr_try = 1\n",
    "    while True:\n",
    "        init = np.floor(np.random.rand(5)*np.array([180,180,180,180,270]))\n",
    "        guess = init[None].T * np.pi/180\n",
    "        R_guess, P_guess = fwdkin(dofbot, guess)\n",
    "       \n",
    "        if P_guess[1] > y_bound:\n",
    "            P_len = np.linalg.norm(P_guess)\n",
    "            if P_len >= limits[0] and P_len <= limits[1]:\n",
    "                print(f\"Generated Position Candidate in {curr_try} tries\")\n",
    "                return guess\n",
    "        curr_try += 1\n",
    "        if curr_try >= tries: break\n",
    "    raise(ValueError, \"No match was found, maybe the y_bound was too strict?\")\n",
    "\n",
    "    # verify that this point is in the bound\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the neural networks\n",
    "\"\"\"\n",
    "The neural network learns an approximation of the value function\n",
    "with a neural network\n",
    "\"\"\"\n",
    "class PPO(nn.Module):\n",
    "\n",
    "    def __init__(self, n_observations, n_actions):\n",
    "        super(PPO, self).__init__()\n",
    "        self.layer1 = nn.Linear(n_observations, 128)\n",
    "        self.layer2 = nn.Linear(128, 128)\n",
    "        self.layer3 = nn.Linear(128, n_actions)\n",
    "\n",
    "    # Called with either one element to determine next action, or a batch\n",
    "    # during optimization. Returns tensor([[left0exp,right0exp]...]).\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        return self.layer3(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import namedtuple, deque\n",
    "\n",
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.linalg import norm\n",
    "def linearity(P_start, P_end, P_curr):\n",
    "    \"\"\"returns the linearity score \n",
    "    (| P12 x P01 |) / |P12|\n",
    "    \"\"\"\n",
    "    P12 = P_end - P_start\n",
    "    P01 = P_start - P_curr\n",
    "    d = norm(np.cross(P12, P01))/norm(P12)\n",
    "    return d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Press the Third Button the DOFBOT\n",
    "(Set the robot to the home position)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "virtual_arm = UR5Arm(P, H, limits)\n",
    "real_arm = Arm_Device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.07, 0.3985291515494993)\n"
     ]
    }
   ],
   "source": [
    "from kinematics import fwdkin, invkin\n",
    "j = read_all_joints_in_radians(real_arm)[None].T\n",
    "R_home, P_home = fwdkin(virtual_arm, j)\n",
    "limits = 0.07, P_home[2]\n",
    "print(limits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train(episodes: int, virtual_arm: UR5Arm, real_arm: Arm, limits):\n",
    "\n",
    "ms_per_angle = 6\n",
    "# coefficient for time weight\n",
    "ee_distance_limits = ()\n",
    "end_effector_distance_tolerance = 1e-4\n",
    "y_bound = 0.07\n",
    "t_coeff = 0.1\n",
    "conv_coef = 0.9\n",
    "l_coeff = 0.1  # weight of linearity score\n",
    "time_per_trial = 10  # in seconds\n",
    "\n",
    "BATCH_SIZE = 128  # BATCH_SIZE is the number of transitions sampled from the replay buffer\n",
    "GAMMA = 0.99      # GAMMA is the discount factor as mentioned in the previous section\n",
    "EPS_START = 0.9   # EPS_START is the starting value of epsilon\n",
    "EPS_END = 0.05    # EPS_END is the final value of epsilon\n",
    "EPS_DECAY = 1000  # EPS_DECAY controls the rate of exponential decay of epsilon, higher means a slower decay\n",
    "TAU = 0.005       # TAU is the update rate of the target network\n",
    "LR = 1e-4         # LR is the learning rate of the ``AdamW`` optimizer\n",
    "\n",
    "# Get number of actions from gym action space\n",
    "n_actions = 5\n",
    "# Get the number of state observations\n",
    "n_observations = 3\n",
    "\n",
    "policy_net = PPO(n_observations, n_actions).to(device)\n",
    "\n",
    "optimizer = optim.AdamW(policy_net.parameters(), lr=LR, amsgrad=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import MultivariateNormal\n",
    "\n",
    "def optimize_model(policy_net, states, actions, old_probs, advantages, epsilon_clip=0.2):\n",
    "    # Convert lists to PyTorch tensors\n",
    "    states = torch.tensor(states, dtype=torch.float32).to(device)\n",
    "    actions = torch.tensor(actions, dtype=torch.float32).to(device)\n",
    "    old_probs = torch.tensor(old_probs, dtype=torch.float32).to(device)\n",
    "    advantages = torch.tensor(advantages, dtype=torch.float32).to(device)\n",
    "\n",
    "    # Get current policy distribution\n",
    "    policy, _ = policy_net(states)\n",
    "    probs = policy.pdf(actions)\n",
    "\n",
    "    # Calculate ratio and surrogate loss\n",
    "    ratio = probs / old_probs\n",
    "    unclipped_loss = ratio * advantages\n",
    "    clipped_loss = torch.clamp(ratio, 1.0 - epsilon_clip, 1.0 + epsilon_clip) * advantages\n",
    "    surrogate_loss = -torch.min(unclipped_loss, clipped_loss).mean()\n",
    "\n",
    "    # Update the policy network\n",
    "    optimizer.zero_grad()\n",
    "    surrogate_loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_input(curr_angles, R_curr, P_curr, target_end_effector_pose):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training Loop for Episode 0\n",
      "Generated Target Norm: 0.3731\n",
      "[0.21046199 0.30357971 0.05275944]\n",
      "P generated: [0.21046199 0.30357971 0.05275944]\n",
      "Generated Target Norm: 0.3052\n",
      "[0.01910396 0.04282973 0.30154195]\n",
      "Retrying: y value not within bounds\n",
      "Generated Target Norm: 0.3003\n",
      "[0.22758848 0.15268135 0.12282767]\n",
      "P generated: [0.22758848 0.15268135 0.12282767]\n",
      "Generated Position Candidate in 2 tries\n",
      "Generated Position Candidate in 2 tries\n",
      "[2.16420827 0.41887902 1.67551608 0.45378561 4.39822972]\n",
      "here [124  24  96  26 252]\n",
      "Command: Joint 1 -> 124 deg in 744 ms\n",
      "Arm_serial_servo_write I2C error\n",
      "Command: Joint 2 -> 24 deg in 144 ms\n",
      "Arm_serial_servo_write I2C error\n",
      "Command: Joint 3 -> 96 deg in 576 ms\n",
      "Arm_serial_servo_write I2C error\n",
      "Command: Joint 4 -> 26 deg in 156 ms\n",
      "Arm_serial_servo_write I2C error\n",
      "Command: Joint 5 -> 252 deg in 1512 ms\n",
      "Arm_serial_servo_write I2C error\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'target_end_effector_pose' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-cf71e8e4722a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mstart_time_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mobservations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurr_angles\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mR_curr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mP_curr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_end_effector_pose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0mdq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpolicy_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'target_end_effector_pose' is not defined"
     ]
    }
   ],
   "source": [
    "# set gripper to position 0\n",
    "real_arm.Arm_serial_servo_write(6, 10, 500)\n",
    "# optimization and ML constants\n",
    "episodes = 100\n",
    "for ep in range(episodes):\n",
    "    print(f\"Starting Training Loop for Episode {ep}\")\n",
    "    \"\"\" generate start and end positions \"\"\"\n",
    "    P_start = generate_P_from_limits(limits, y_bound)\n",
    "    P_end = generate_P_from_limits(limits, y_bound)\n",
    "    \n",
    "    Q_start = generate_Q_from_limits(virtual_arm, limits, y_bound)\n",
    "    Q_end = generate_Q_from_limits(virtual_arm, limits, y_bound)\n",
    "\n",
    "    # initialize the arm to a random state. \n",
    "    print(Q_start.flatten())\n",
    "    set_angles(real_arm, Q_start.flatten(), ms_per_angle)\n",
    "    \n",
    "    # allow the arm to move for 5 seconds\n",
    "    time_start = time.time()\n",
    "    current_time = 0\n",
    "    reward = 0\n",
    "\n",
    "    curr_angles = read_all_joints_in_radians(real_arm)\n",
    "    R_curr, P_curr = fwdkin(virtual_arm, curr_angles[None].T)\n",
    "    exit(1)\n",
    "    while(current_time - time_start < time_per_trial):\n",
    "        start_time_step = time.time()\n",
    "\n",
    "        observations = create_input(curr_angles, R_curr, P_curr, target_end_effector_pose)\n",
    "        dq = policy_net(observations).T\n",
    "\n",
    "        next_angles = curr_angles + dq\n",
    "        dofbot_limits = virtual_arm.get_limits()\n",
    "        if next_angles.any() > dofbot_limits[:,0]:\n",
    "            next_angles = max(dofbot_limits[:,0], next_angles)\n",
    "            reward -= 10\n",
    "            \n",
    "        if next_angles.any() < dofbot_limits[:,1]:\n",
    "            next_angles = min(dofbot_limits[:,1], next_angles)\n",
    "            reward -= 10\n",
    "            \n",
    "        set_angles(real_arm, next_angles, dq * ms_per_angle)\n",
    "\n",
    "        next_angles = read_all_joints()\n",
    "        R_next, P_next = fwdkin(dofbot, curr_angles)\n",
    "        \n",
    "        current_time = time.time()\n",
    "        elapsed_time = current_time - start_time_step\n",
    "        \n",
    "        dist = np.linalg.norm(P_end - P_curr) \n",
    "        lin_score = linearity(P_start, P_end, P_curr)\n",
    "        # loss should be based on the euclidean distance, time it took, and linearity of the path\n",
    "        reward -= conv_coef * dist - tcoef * elapsed_time - l_coef * lin_score\n",
    "\n",
    "        \"\"\"\n",
    "        TODO: add replay Memory for stability\n",
    "        memory.push(state, action, next_state, reward)\n",
    "        \n",
    "        TODO: add replayMemory for stability\n",
    "        optimize_model_batch\n",
    "        \"\"\"\n",
    "        \n",
    "        if dist.all() < end_effector_distance_tolerance:\n",
    "            break\n",
    "\n",
    "        # PPO update without replay buffer\n",
    "        \"\"\" \n",
    "        To-do \n",
    "        check this and see what this is doing\n",
    "        \"\"\"\n",
    "        optimize_model(policy_net, observations, dq, old_probs, advantages)\n",
    "        \n",
    "        current_time = time.time()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
